{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"c2w5_HomeWork_1NN_vs_RF.ipynb","provenance":[],"authorship_tag":"ABX9TyP3xA8VEhlN9f8NiywHe75L"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Oe1EwfjF37Sg"},"source":["#1NN против RandomForest"]},{"cell_type":"markdown","metadata":{"id":"--mktuEQ39rq"},"source":["В этом задании будет использоваться датасет digits из sklearn.datasets. Оставьте последние 25% объектов для контроля качества, разделив X и y на X_train, y_train и X_test, y_test.\n","\n","Целью задания будет реализовать самый простой метрический классификатор — метод ближайшего соседа, а также сравнить качество работы реализованного вами 1NN с RandomForestClassifier из sklearn на 1000 деревьях."]},{"cell_type":"code","metadata":{"id":"Aszn2tzM8SKJ","executionInfo":{"status":"ok","timestamp":1632053049892,"user_tz":-180,"elapsed":206,"user":{"displayName":"Александр Апраксин","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17818450637708548496"}}},"source":["# для записи ответов\n","def write_answer(answer, file_name):\n","    with open(file_name, 'w') as fout:\n","        fout.write(answer)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kkmIjav04crL","executionInfo":{"status":"ok","timestamp":1632053601109,"user_tz":-180,"elapsed":201,"user":{"displayName":"Александр Апраксин","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17818450637708548496"}},"outputId":"37fbc61b-5d01-44ca-dd4e-6d472aac4a32"},"source":["from sklearn.datasets import load_digits\n","\n","digits = load_digits()\n","X = digits.data\n","y = digits.target\n","\n","split = int(X.shape[0] * 0.75)\n","\n","X_train, X_test = X[:split], X[split:]\n","y_train, y_test = y[:split], y[split:]\n","\n","print(X.shape, y.shape, split)\n","print(X_train.shape, y_train.shape)\n","print(X_test.shape, y_test.shape)"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["(1797, 64) (1797,) 1347\n","(1347, 64) (1347,)\n","(450, 64) (450,)\n"]}]},{"cell_type":"markdown","metadata":{"id":"E4RqJxJV4CfQ"},"source":["##Задание 1\n","\n","Реализуйте самостоятельно метод одного ближайшего соседа с евклидовой метрикой для задачи классификации. Можно не извлекать корень из суммы квадратов отклонений, т.к. корень — монотонное преобразование и не влияет на результат работы алгоритма.\n","\n","Никакой дополнительной работы с признаками в этом задании делать не нужно — мы еще успеем этим заняться в других курсах. Ваша реализация может быть устроена следующим образом: можно для каждого классифицируемого объекта составлять список пар (расстояние до точки из обучающей выборки, метка класса в этой точке), затем сортировать этот список (по умолчанию сортировка будет сначала по первому элементу пары, затем по второму), а затем брать первый элемент (с наименьшим расстоянием).\n","\n","Сортировка массива длиной N требует порядка N log N сравнений (строже говоря, она работает за O(N log N)). Подумайте, как можно легко улучшить получившееся время работы. Кроме простого способа найти ближайший объект всего за N сравнений, можно попробовать придумать, как разбить пространство признаков на части и сделать структуру данных, которая позволит быстро искать соседей каждой точки. За выбор метода поиска ближайших соседей в KNeighborsClassifier из sklearn отвечает параметр algorithm — если у вас уже есть некоторый бэкграунд в алгоритмах и структурах данных, вам может быть интересно познакомиться со структурами данных ball tree и kd tree.\n","\n","Доля ошибок, допускаемых 1NN на тестовой выборке, — ответ в задании 1."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sTd_Ph3a39Ac","executionInfo":{"status":"ok","timestamp":1632055078855,"user_tz":-180,"elapsed":207,"user":{"displayName":"Александр Апраксин","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17818450637708548496"}},"outputId":"a52894fc-e8c3-41a3-9b9f-693c9991cb73"},"source":["%%time\n","import sklearn\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score\n","\n","knn_clf = KNeighborsClassifier(n_neighbors=1)\n","knn_clf.fit(X_train, y_train)\n","\n","ans1 = 1 - accuracy_score(y_test, knn_clf.predict(X_test))\n","print(ans1)\n","write_answer(str(ans1), '1nn_vs_rf_ans1.txt')"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["0.0377777777777778\n","CPU times: user 99 ms, sys: 0 ns, total: 99 ms\n","Wall time: 100 ms\n"]}]},{"cell_type":"code","metadata":{"id":"SgkYvI9iCeya"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cc31_XX54J-w"},"source":["##Задание 2\n","\n","Теперь обучите на обучающей выборке RandomForestClassifier(n_estimators=1000) из sklearn. Сделайте прогнозы на тестовой выборке и оцените долю ошибок классификации на ней. Эта доля — ответ в задании 2. Обратите внимание на то, как соотносится качество работы случайного леса с качеством работы, пожалуй, одного из самых простых методов — 1NN. Такое различие — особенность данного датасета, но нужно всегда помнить, что такая ситуация тоже может иметь место, и не забывать про простые методы."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZhnuhPe83lC_","executionInfo":{"status":"ok","timestamp":1632055140399,"user_tz":-180,"elapsed":3769,"user":{"displayName":"Александр Апраксин","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17818450637708548496"}},"outputId":"312cda3b-2d36-4499-a314-49afe74cd037"},"source":["%%time\n","from sklearn.ensemble import RandomForestClassifier\n","\n","rf_clf = RandomForestClassifier(n_estimators=1000)\n","rf_clf.fit(X_train, y_train)\n","\n","ans2 = 1 - accuracy_score(y_test, rf_clf.predict(X_test))\n","print(ans2)\n","write_answer(str(ans2), '1nn_vs_rf_ans2.txt')"],"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["0.06666666666666665\n","CPU times: user 3.5 s, sys: 13.8 ms, total: 3.52 s\n","Wall time: 3.51 s\n"]}]},{"cell_type":"code","metadata":{"id":"B91FxN-XDNgZ"},"source":[""],"execution_count":null,"outputs":[]}]}